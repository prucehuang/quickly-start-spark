package com.gitlab.huanghaifeng.spark.sparkstreaming
/**
 * @description
 * 监听localhost的7777端口，对每一行的输入进行计数
 * 并且将结果数据存到本地文件中
 * 每次执行都会将当前的RDD计算结果 存到 本地
 * nc -lk 7777 开启端口发送数据
 * 运行代码
 * spark-submit --class com.hhf.spark.streaming.StreamingSocket2 ./sparkStreamingExample.jar 192.168.9.223 7777
 * @version V1.0
 * @author HHF
 * @time 2016-5-24
 */
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.SparkConf
import org.apache.spark.streaming.Minutes
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.Minutes

object StreamingSocket2 {
  
    def main(args: Array[String]) {
        if (args.length < 2) {
            System.err.println("Usage: NetworkWordCount <hostname> <port>")
            System.exit(1)
        }

        // Create the context with a 1 second batch size
        val sparkConf = new SparkConf().setAppName("NetworkWordCount")
        val ssc = new StreamingContext(sparkConf, Minutes(1))

        // Create a socket stream on target ip:port and count the
        // words in input stream of \n delimited text (eg. generated by 'nc')
        // Note that no duplication in storage level only for running locally.
        // Replication necessary in distributed scenario for fault tolerance.
        val lines = ssc.socketTextStream(args(0), args(1).toInt)//, StorageLevel.MEMORY_AND_DISK_SER)
        val words = lines.flatMap(_.split(" "))
        val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
        wordCounts.print()
//        wordCounts.saveAsTextFiles("/Users/huanghaifeng/Documents/spark/jobs/output", "result")
        ssc.start()
        // Wait for 10 seconds then exit. To run forever call without a timeout
//        ssc.awaitTermination(10000)
        ssc.stop()
    }
}